# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KaDQxhRFICZ7P4P1dnct7JbQG3tzZ3jL
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

F_employement= pd.read_csv("Data.csv")

F_employement = F_employement[(F_employement['yearBirth'] >= 1950) & (F_employement['yearBirth'] <= 2007)]

F_employement = F_employement[F_employement['FemaleFlag'] != 0]



# Ensure the DataFrame includes all rows by checking the total number of rows before and after the split
total_rows_before_split = F_employement.shape[0]

split_value = 1489039200
# Perform the split again to include the clarification
higher_df = F_employement[F_employement['api_applications_created'] >= split_value]
lower_df = F_employement[F_employement['api_applications_created'] < split_value]

# Verify that all rows are included in the split
total_rows_after_split = higher_df.shape[0] + lower_df.shape[0]

total_rows_before_split, total_rows_after_split

#higher_df.head(50)
lower_df.head(50)

X = higher_df[['yearBirth', 'course_type', 'job_role']]

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler

# Assuming 'higher_df' is your DataFrame with the combined dataset

# Filling missing numeric data with the median of the respective column
higher_df.fillna(higher_df.median(), inplace=True)

# Selecting features and the target variable
X = higher_df.drop(['id', 'client_id', 'job_id', 'fid', 'title', 'date_of_posting', 'call_letter_date', 'bd_lnkdin_co_profile_emp2', 'CalLetterFlag'], axis=1)
y = higher_df['CalLetterFlag']

# Convert categorical variables into dummy variables. This step is crucial to convert all text data into numeric form.
X = pd.get_dummies(X, drop_first=True)

# Filling any remaining missing values with 0 (after potential median imputation)
X.fillna(0, inplace=True)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initializing and fitting the logistic regression model with class weights for balancing
model = LogisticRegression(class_weight='balanced')
model.fit(X_train_scaled, y_train)

# Making predictions on the test set
y_pred = model.predict(X_test_scaled)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Printing the evaluation results
print(f"Accuracy: {accuracy}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

filename = "model.pickle"
pickle.dump(model, open(filename, 'wb'))
loaded_model = pickle.load(open(filename, 'rb'))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler

# Assuming 'higher_df' is your DataFrame with the combined dataset

# Filling missing numeric data with the median of the respective column
lower_df.fillna(lower_df.median(), inplace=True)

# Selecting features and the target variable
X = lower_df.drop(['id', 'client_id', 'job_id', 'fid', 'title', 'date_of_posting', 'call_letter_date', 'bd_lnkdin_co_profile_emp2', 'CalLetterFlag'], axis=1)
y = lower_df['CalLetterFlag']

# Convert categorical variables into dummy variables. This step is crucial to convert all text data into numeric form.
X = pd.get_dummies(X, drop_first=True)

# Filling any remaining missing values with 0 (after potential median imputation)
X.fillna(0, inplace=True)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initializing and fitting the logistic regression model with class weights for balancing
model1 = LogisticRegression(class_weight='balanced')
model1.fit(X_train_scaled, y_train)

# Making predictions on the test set
y_pred = model1.predict(X_test_scaled)

# Evaluating the model1
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Printing the evaluation results
print(f"Accuracy: {accuracy}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

filename = "model1.pickle"
pickle.dump(model1, open(filename, 'wb'))
loaded_model = pickle.load(open(filename, 'rb'))

type(X_test)

print(X_test)